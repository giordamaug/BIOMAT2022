# -*- coding: utf-8 -*-
"""NodeClassificaytion.ipynb



Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NbmTr3i8M5BIn9UkOuNEY4xP81_EbbZh
"""

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    
import argparse
from operator import index
import numpy as np
from tqdm import tqdm

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_selector as selector
from sklearn.neural_network import MLPClassifier
import sys
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold, KFold, train_test_split
from sklearn.metrics import *
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier
from imblearn.ensemble import RUSBoostClassifier
from sklearn.dummy import DummyClassifier
from lightgbm import LGBMClassifier
from logitboost import LogitBoost
from tabulate import tabulate
from progress.counter import Stack

parser = argparse.ArgumentParser(description='BIOMAT 2022 Workbench')
parser.add_argument('-a', "--attributes", dest='attributes', metavar='<attributes>', nargs="+", default=["BIO", "EMBED"], choices=["BIO", "EMBED"], help='attributes to consider (default BIO EMBED, values BIO,EMBED)', required=False)
parser.add_argument('-x', "--excludelabels", dest='excludelabels', metavar='<excludelabels>', nargs="+", default=[], help='labels to exclude (default NaN, values any list)', required=False)
parser.add_argument('-i', "--onlyattributes", dest='onlyattributes', metavar='<onlyattributes>', nargs="+", default=None, help='attributes to use (default All, values any list)', required=False)
parser.add_argument('-o', "--excludeattributes", dest='excludeattributes', metavar='<excludeattributes>', nargs="+", default=None, help='attributes to exlude (default None, values any list)', required=False)
parser.add_argument('-c', "--embeddir", dest='embeddir', metavar='<embedding-dir>', type=str, help='embedding directory (default embeddings)', default='embeddings', required=False)
parser.add_argument('-d', "--datadir", dest='datadir', metavar='<data-dir>', type=str, help='data directory (default datasets)', default='datasets', required=False)
parser.add_argument('-l', "--labelname", dest='labelname', metavar='<labelname>', type=str, help='label name (default label_CS_ACH_most_freq)', default='label_CS_ACH_most_freq', required=False)
parser.add_argument('-F', "--labelfile", dest='labelfile', metavar='<labelfile>', type=str, help='label filename (default multiLabels.csv)', default='multiLabels.csv', required=False)
parser.add_argument('-A', "--attrfile", dest='attrfile', metavar='<attrfile>', type=str, help='attribute filename (default integratedNet_nodes_bio.csv)', default='integratedNet_nodes_bio.csv', required=False)
parser.add_argument('-B', "--seqfile", dest='seqfile', metavar='<seqfile>', type=str, help='attribute filename', required=False)
parser.add_argument('-P', "--netfile", dest='netfile', metavar='<netfile>', type=str, help='network filename (default ppi_edges.csv)', default='ppi_edges.csv', required=False)
parser.add_argument('-n', "--network", dest='network', metavar='<network>', type=str, help='network (default: PPI, choice: PPI|MET|MET+PPI)', choices=['PPI', 'MET', 'MET+PPI'], default='PPI', required=False)
parser.add_argument('-Z', "--normalize", dest='normalize', metavar='<normalize>', type=str, help='normalize mode (default: None, choice: None|zscore|minmax)', choices=[None, 'zscore', 'minmax'], default=None, required=False)
parser.add_argument('-I', "--imputation", dest='imputation', metavar='<imputation>', type=str, help='imputation mode (default: None, choice: None|mean|zero)', choices=[None, 'mean', 'zero'], default=None, required=False)
parser.add_argument('-e', "--embedder", dest='embedder', metavar='<embedder>', type=str, help='embedder name (default: RandNE, choice: RandNE|Node2Vec|GLEE|DeepWalk|HOPE|... any other)' , default='RandNE', required=False)
parser.add_argument('-s', "--embedsize", dest='embedsize', metavar='<embedsize>', type=int, help='embed size (default: 128)' , default='128', required=False)
parser.add_argument('-b', "--seed", dest='seed', metavar='<seed>', type=int, help='seed (default: 1)' , default='1', required=False)
parser.add_argument('-m', "--method", dest='method', metavar='<method>', type=str, help='classifier name (default: RF, choice: RF|SVM|XGB|LGBM|MLP|LG|EXT|VC)', choices=['VC', 'RF', 'RUS', 'SVM', 'XGB', 'LGBM', 'EXT', 'MLP', 'WNN', 'LG'], default='RF', required=False)
parser.add_argument('-D', "--tocsv", dest="tocsv", type=str, required=False)
parser.add_argument('-O', "--tuneparams", dest='tuneparams',  action='store_true', required=False)
parser.add_argument('-S', "--saveembeddingfile", dest='saveembeddingfile', type=str, required=False)
parser.add_argument('-L', "--embeddingfile", dest='embeddingfile',  type=str, required=False)
parser.add_argument('-X', "--display", action='store_true', required=False)
parser.add_argument('-G', "--proba", action='store_true', required=False)
parser.add_argument('-Q', "--removefeat", action='store_true', required=False)
args = parser.parse_args()

seed=args.seed



import warnings
warnings.filterwarnings('ignore')
import random
import pandas as pd
def set_seed(seed=1):
    random.seed(seed)
    np.random.seed(seed)

"""# Load the network"""

#@title  { run: "auto", form-width: "30%" }
network = args.network #@param ["PPI", "Met", "Met+PPI"]
import sys
if 'google.colab' in sys.modules:
	import tqdm.notebook as tq
else:
	import tqdm as tq
import pandas as pd
import networkx as nx
import os

datapath = args.datadir

"""# Read the labels
Load the label file, select the label type, and print label distribution
"""

from sklearn import preprocessing
from collections import Counter
import matplotlib.pyplot as plt

#@title Testo del titolo predefinito { run: "auto", form-width: "20%" }
label_filename = args.labelfile #@param {type:"string"}
labelname = args.labelname #@param {type:"string"}
exclude_labels = [np.nan] + args.excludelabels #@param {type:"raw"}
#label_aliases = {"aE": "E", "aNE" : "NE"}
label_aliases = {}
label_file = os.path.join(datapath, label_filename)
df_label = pd.read_csv(label_file, sep=',', index_col=0)
if labelname in df_label.columns:
    print(bcolors.HEADER + f'Loading label {labelname} from file "{label_file}"...' + bcolors.ENDC)
else:
    print(bcolors.FAIL + f'FAIL: Label name {labelname} is not in the label file{label_filename}!' + bcolors.ENDC)
print(bcolors.OKCYAN + f'Labeling...' + bcolors.ENDC)
print(bcolors.OKGREEN + f'- working on label "{labelname}""...' + bcolors.ENDC)
dup = df_label.index.duplicated().sum()
if dup > 0:
    df_label = df_label[~df_label.index.duplicated(keep='first')]
    print(bcolors.OKGREEN + f'- removing {dup} duplicated genes...' + bcolors.ENDC)
genes = df_label.index.values                                                                    # get genes with defined labels
df_label = df_label[df_label[labelname].isin([np.nan] + exclude_labels) == False]                # drop any row contaning NaN or SC1-SC5 as value
labels = np.unique(df_label[labelname].values)
print(bcolors.OKGREEN + f'- used label values {labels} (excluded {exclude_labels})' + bcolors.ENDC)
for key,newkey in label_aliases.items():
    if key in labels:
        print(bcolors.OKGREEN + f'- replacing label {key} with {newkey}' + bcolors.ENDC)
        df_label = df_label.replace(key, newkey)
selectedgenes = df_label.index.values
print(bcolors.OKGREEN + f'- {len(selectedgenes)} labeled genes over a total of {len(genes)}' + bcolors.ENDC)
print(bcolors.OKGREEN + f'- label distribution: {dict(df_label[labelname].value_counts())}' + bcolors.ENDC)


"""# Load attributes to be used
We identified three sets of attributes:
1. bio attributes, related to gene information (such as, expression, etc.)
Based on user selection, the node attributes are appended in a single matrix of attributes (`x`)

In the attribute matrix `x` there can be NaN or Infinite values. They are corrected as it follow:
+ NaN is replaced by the mean in the attribute range, 
+ Infinte value is replaced by the maximum in the range.

After Nan and Infinite values fixing, the attributes are normalized with Z-score or MinMax normalization functions.

At the end, only nodes (genes) with E or NE labels are selected for the classification
"""

#@title Choose attributes { form-width: "20%" }
def intersection(lst1, lst2):
    lst3 = [value for value in lst1 if value in lst2]
    return lst3

if "BIO" in args.attributes:
  attr_file = os.path.join(datapath,args.attrfile)
  print(bcolors.HEADER + f'Loading attribute matrix "{attr_file}"...' + bcolors.ENDC)
  x = pd.read_csv(attr_file, index_col=0)
  x = x.select_dtypes(include=np.number)     # drop non numeric attributes
  orignrows = x.shape[0]
  print(bcolors.OKCYAN + f'Filtering attributes...' + bcolors.ENDC)
  if args.onlyattributes is not None:
    x = x.filter(items=args.onlyattributes)
  if args.excludeattributes is not None:
    x = x.drop(columns=intersection(args.excludeattributes, list(x.columns)))
  print(bcolors.OKCYAN + f'Statistics...' + bcolors.ENDC)
  print(bcolors.OKGREEN + f'- found {len(x.columns)} data columns' + bcolors.ENDC)
  dup = x.index.duplicated().sum()
  constcolumns = list(x.loc[:, x.apply(pd.Series.nunique)==1].columns)
  nancolumns = x.columns[x.isna().all()].tolist()
  print(bcolors.OKGREEN + f'- {dup} duplicated genes (removed!)' + bcolors.ENDC)
  x = x[~x.index.duplicated(keep='first')]   # remove eventually duplicated index
  if dup > 0:
      x = x[~x.index.duplicated(keep='first')]
  print(bcolors.OKGREEN + f'- {len(nancolumns)} null columns (removed!)' + bcolors.ENDC)
  if len(nancolumns) > 0:
      x = x.drop(nancolumns, axis=1)
  print(bcolors.OKGREEN + f'- {len(constcolumns)} constant columns (removed!)' + bcolors.ENDC)
  if len(constcolumns) > 0:
      x = x.drop(constcolumns, axis=1)
  bincolnames = list(x.loc[:, x.isin([0.0,1.0, np.nan]).all()].columns)
  nbinary = len(bincolnames) if len(bincolnames) > 1 else 0
  print(bcolors.OKGREEN + f'- {nbinary} binary attributes (no normalization!)' + bcolors.ENDC)

  nancount = x.isnull().sum().sum()
  print(bcolors.OKGREEN + f'- {nancount} null values' + bcolors.ENDC)
  ninf = np.isinf(x).values.sum()
  print(bcolors.OKGREEN + f'- {ninf} Infinite values' + bcolors.ENDC)

  realcolumns = list(set(x.columns) - set(bincolnames)) 

  def impute(x, realcolumns, bincolumns, mode):
    df = x.copy()
    #print(bcolors.OKGREEN + f'- fixing {int(df[realcolumns].isnull().sum().sum())} null values in {len(realcolumns)}/{len(df.columns)} real columns ... ' + bcolors.ENDC, end='')
    p = Stack(bcolors.OKGREEN + f'- fixing {int(df[realcolumns].isnull().sum().sum())} null values in {len(realcolumns)}/{len(df.columns)} real columns ... ' + bcolors.ENDC, max=len(realcolumns))
    if mode == "mean":
        for col in realcolumns: 
          mean_value= df[col].mean()
          if np.isnan(mean_value):
            df = df.drop(columns=[col])
          else:
            df[col].fillna(value=mean_value, inplace=True)
          p.next()
        print(bcolors.OKGREEN + " done!" + bcolors.ENDC,)
    elif mode == "zero":
        for col in df.columns: 
          df[col].fillna(value=0, inplace=True)
        p.next()
        print(bcolors.OKGREEN + " done!" + bcolors.ENDC,)
    else:
      raise Exception(f'Imputation not supported "{mode}"!')
    #print(bcolors.OKGREEN + f'- fixing {int(df[bincolumns].isnull().sum().sum())} null values in {len(bincolumns)}/{len(df.columns)} binary columns' + bcolors.ENDC, end='')
    p = Stack(bcolors.OKGREEN + f'- fixing {int(df[bincolumns].isnull().sum().sum())} null values in {len(bincolumns)}/{len(df.columns)} binary columns ... ' + bcolors.ENDC, max=len(bincolumns))
    for col in bincolumns:
      mostcommon = df[col].mode()[0] if len(df[col].mode()) > 0 else 0
      df[col].fillna(value=mostcommon, inplace=True)
      p.next()
    if len(bincolumns) >  0: print(bcolors.OKGREEN + " done!" + bcolors.ENDC,) 
    print(bcolors.OKGREEN + f'- remaining {df.isnull().sum().sum()} null values' + bcolors.ENDC)
    return df

  def normalize(x, columns, mode):
    df = x.copy()
    #print(bcolors.OKGREEN + f'- applying normalization to {len(columns)} columns' + bcolors.ENDC, end='')
    p = Stack(bcolors.OKGREEN + f'- applying normalization to {len(columns)} columns ... ' + bcolors.ENDC, max=len(columns))
    for col in columns:
      if mode == 'minmax':
          df[col] = (df[col]-df[col].min())/(df[col].max()-df[col].min())
      elif args.normalize == 'zscore':
          df[col] = (df[col]-df[col].mean())/df[col].std()
      else:
          raise Exception(f'Imputation not supported "{mode}"!')
      p.next()
    print(bcolors.OKGREEN + " done!" + bcolors.ENDC,)
    return df

  if args.imputation is not None:
    print(bcolors.OKCYAN + f'Imputation with "{args.imputation}"...' + bcolors.ENDC)
    x = impute(x, realcolumns, bincolnames, args.imputation)

  if args.normalize is not None:
    print(bcolors.OKCYAN + f'Normalization with "{args.normalize}"...' + bcolors.ENDC)
    x = normalize(x, realcolumns, args.normalize) 
    print(bcolors.OKGREEN + f'- skipping {nbinary} binary columns' + bcolors.ENDC)

  selectedgenes = intersection(x.index.to_list(), selectedgenes)
  print(bcolors.OKCYAN + f'Reduction...' + bcolors.ENDC)
  print(bcolors.OKGREEN + f'- dataset reduced from {orignrows} to {len(selectedgenes)} genes' + bcolors.ENDC)
  x = x.loc[selectedgenes]
  print(bcolors.OKGREEN + f'- new attribute matrix x{x.shape}' + bcolors.ENDC)
  from pprint import pformat
  from textwrap import indent
  #print(bcolors.OKGREEN + f'- using attributes:\n' + indent(pformat(list(x.columns)),'') + bcolors.ENDC) if len(list(x.columns)) < 50 else None
else:
  x = pd.DataFrame()

if "EMBED" in args.attributes:
  """# Network embedding with Karateclub""" 

  if args.embeddingfile is not None:
    embedfilename = os.path.join(args.embeddir,args.embeddingfile)
    try:
      embedding_df = pd.read_csv(embedfilename, index_col=0)
      print(bcolors.HEADER + f'Loading embedding from file "{embedfilename}"' + bcolors.ENDC)
    except:
      raise Exception(bcolors.FAIL + f"{embedfilename} is not a valid file!" + bcolors.ENDC)
  else:
    from karateclub.node_embedding import *
    embeddername = args.embedder #@param ["RandNE", "Node2Vec", "GLEE", "DeepWalk"]
    if not embeddername in dir():
      raise Exception(bcolors.FAIL + f"{embeddername} is not an embedding method supported in karateclub" + bcolors.ENDC)
    print(bcolors.HEADER + f'Embedding with method "{embeddername}"...' + bcolors.ENDC)
    import networkx as nx
    netfile = os.path.join(datapath,args.netfile)
    df_net = pd.read_csv(netfile, index_col=0)
    print(bcolors.OKGREEN + f'Loading "{network}" network file "{netfile}" ...' + bcolors.ENDC)
    allgenes = np.union1d(df_net["source"].values,df_net["target"].values)       # get genes with defined labels
    indices = np.arange(len(allgenes))
    gene2idx_mapping = { g : i  for g,i in zip(allgenes,indices) }             # create mapping index by gene name
    idx2gene_mapping = { i : g  for g,i in zip(allgenes,indices) }             # create mapping index by gene name

    if network == "PPI":
      G = nx.Graph()
    else:
      G = nx.DiGraph()
    G.add_nodes_from(range(len(allgenes)))                                       # add all nodes (genes, also isolated ones)
    if 'weight' in list(df_net.columns):
      edge_list = [(gene2idx_mapping[v[0]], gene2idx_mapping[v[1]], v[2]) for v in list(df_net[['source','target', 'weight']].values)]      # get the edge list (with weights)
      G.add_weighted_edges_from(edge_list)                                      # add all edges
    else:
      edge_list = [(gene2idx_mapping[v[0]], gene2idx_mapping[v[1]]) for v in list(df_net[['source','target']].values)]    # get the edge list (with weights)
      G.add_edges_from(edge_list)                                      # add all edges
    print(bcolors.OKGREEN + "" + nx.info(G)  + bcolors.ENDC)
    print(bcolors.OKGREEN + f'There are {len(list(nx.isolates(G)))} isolated genes' + bcolors.ENDC)
    print(bcolors.OKGREEN + f'Graph {"is" if nx.is_weighted(G) else "is not"} weighted' + bcolors.ENDC)
    print(bcolors.OKGREEN + f'Graph {"is" if nx.is_directed(G) else "is not"} directed' + bcolors.ENDC)
    embedder = globals()[embeddername](dimensions=args.embedsize)
    embedder.fit(G)
    embedding = embedder.get_embedding()
    embedding_df = pd.DataFrame(embedding, columns = [f'{embeddername}_' + str(i + 1)  for i in range(embedding.shape[1])])
    embedding_df['name'] = [idx2gene_mapping[item] for item in embedding_df.index.values]
    embedding_df = embedding_df.set_index('name')
  if args.saveembeddingfile:
    saveembedfilename = os.path.join(args.embeddir,args.saveembeddingfile)
    embedding_df.to_csv(saveembedfilename)
    print(bcolors.OKGREEN + f'Saving embedding to file "{saveembedfilename}"' + bcolors.ENDC)
  print(bcolors.OKCYAN + f'PPI reduction...' + bcolors.ENDC)
  dup = embedding_df.index.duplicated().sum()
  if dup > 0:
      print(bcolors.OKGREEN + f'- removing {dup} duplicated genes in PPI...' + bcolors.ENDC)
      embedding_df = embedding_df[~embedding_df.index.duplicated(keep='first')]
  selectedgenes = intersection(selectedgenes, embedding_df.index.to_list())
  print(bcolors.OKGREEN + f'- genes found in network are {len(selectedgenes)}' + bcolors.ENDC)
  embedding_df = embedding_df.loc[selectedgenes]                                     # keep only embeddings of selected genes (those with labels)
  x = x.loc[selectedgenes] if not x.empty else x                                   # keep only embeddings of selected genes (those with labels)
  x = embedding_df if x.empty else pd.concat([embedding_df, x], axis=1) 
  print(bcolors.OKGREEN + f'- new attribute matrix x{x.shape}' + bcolors.ENDC)


# print label distribution
from collections import Counter, OrderedDict
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
df_label_reduced = df_label.loc[selectedgenes]
labels = df_label_reduced[labelname].values
distrib = Counter(labels)
encoder = LabelEncoder()
y = encoder.fit_transform(labels)  
classes_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
rev_classes_mapping = np.array(list(classes_mapping.keys()))

plt.pie(list(distrib.values()), labels=list(distrib.keys()), autopct='%2.1f%%', startangle=90)
distrib = dict(df_label_reduced[labelname].value_counts())
print(bcolors.OKGREEN + f'- new label distribution:  {distrib}' + bcolors.ENDC)

### set balancing in method paramters
classifier_map = {'RF' : 'RandomForestClassifier', 
                  'MLP': 'MLPClassifier', 
                  'SVM' : 'SVC', 
                  'RUS':  'RUSBoostClassifier',
                  'XGB': 'XGBClassifier',
                  'EXT': 'ExtraTreesClassifier',
                  'LGBM': 'LGBMClassifier',
                  'VC' : 'VotingClassifier',
                  'LG': 'LogitBoost'}

if args.tuneparams:
  classifiers_args = {
    'RF' : {'random_state' : seed, 'class_weight': 'balanced'}, 
    'MLP': {'random_state' : seed}, 
    'SVM' : {'random_state' : seed, 'class_weight': 'balanced'}, 
    'RUS': {'random_state' : seed},
    'LGBM': {'random_state' : seed, 'class_weight': 'balanced'},
    'EXT': {'random_state' : seed, 'class_weight': 'balanced'},
    'XGB': {'random_state' : seed, 'objective' : "binary:logistic", 'eval_metric' : 'logloss', },
    'VC' : {'estimators': [
        ('xt', ExtraTreesClassifier(random_state = seed, class_weight = 'balanced')), 
        ('xgb', XGBClassifier(random_state = seed, objective = "binary:logistic", eval_metric = 'logloss',  scale_pos_weight=0.2)), 
        ('lgb', LGBMClassifier(random_state = seed, class_weight='balanced'))],
         'voting' :'soft'},
    'LG' : {'random_state' : seed, 'n_estimators':200 }
  }
  if len(distrib.keys()) == 2:
    first, second = list(distrib.values())
    if first < second:
      factor = round(float(first)/float(second), 2)
    else:
      factor = round(float(second)/float(first), 2)
    classifiers_args["XGB"] = {'random_state' : seed, 'objective' : "binary:logistic", 'eval_metric' : 'logloss', 'scale_pos_weight' : factor}
else:
  classifiers_args = {
    'RF' : {'random_state' : seed}, 
    'MLP': {'random_state' : seed}, 
    'SVM' : {'random_state' : seed}, 
    'RUS': {'random_state' : seed},
    'LGBM': {'random_state' : seed},
    'EXT': {'random_state' : seed},
    'XGB': {'random_state' : seed},
    'VC' : {'estimators': [
        ('xt', ExtraTreesClassifier(random_state = seed)), ('xgb', XGBClassifier(random_state = seed, objective = "binary:logistic", eval_metric = 'logloss')), ('lgb', LGBMClassifier(random_state = seed))],
         'voting' :'hard'},
    'LG' : {'random_state' : seed}
  }

## check method comaptibility
if x.isnull().sum().sum() > 0 and args.method not in ['LGBM']:
  print(bcolors.FAIL + f'ERR: Method "{args.method}" does not support NaN or Inf input values ... try using -I <imputation-mode>!' + bcolors.ENDC)
  sys.exit(-1)
"""# k-fold cross validation with: SVM, RF, XGB, MLP, RUS

"""

method = args.method #@param ["SVM", "XGB", "RF", "MLP", "RUS", "LGB"]
set_seed(seed)
nfolds = 5
kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=seed)
accuracies, mccs = [], []
genes = x.index.values
X = x.to_numpy()
if args.tocsv is not None:
  newd = x.copy()
  #newd['class'] = y
  newd.to_csv(os.path.join(datapath,args.tocsv), index=True)
  print(bcolors.HEADER + f'Saving dataset to file {args.tocsv}' + bcolors.ENDC)

if args.removefeat:
  print(bcolors.HEADER + f'Selecting features ...' + bcolors.ENDC)
  from sklearn.feature_selection import RFE
  from sklearn.svm import SVR
  estimator = SVR(kernel="linear")
  selector = RFE(estimator, n_features_to_select=100, step=1)
  X = selector.fit_transform(X, y)
  print(bcolors.OKGREEN + f'New attribute matrix x{X.shape}' + bcolors.ENDC)

nclasses = len(classes_mapping)
cma = np.zeros(shape=(nclasses,nclasses), dtype=np.int)
mm = np.array([], dtype=np.int)
gg = np.array([])
yy = np.array([], dtype=np.int)
probabilities = np.array([])
predictions = np.array([], dtype=int)
columns_names = ["Accuracy","BA", "Sensitivity", "Specificity","MCC", 'CM']
scores = pd.DataFrame(columns=columns_names)
print(bcolors.HEADER + f'Classification with method "{method}"...' + bcolors.ENDC)
print(bcolors.OKGREEN + f'- classifier params: {classifiers_args[method]}' + bcolors.ENDC)
for fold, (train_idx, test_idx) in enumerate(tqdm(kf.split(np.arange(len(X)), y), total=kf.get_n_splits(), desc=bcolors.OKGREEN +  f"{nfolds}-fold")):
    train_x, train_y, test_x, test_y = X[train_idx], y[train_idx], X[test_idx], y[test_idx],
    mm = np.concatenate((mm, test_idx))
    yy = np.concatenate((yy, test_y))
    gg = np.concatenate((gg, genes[test_idx]))
    clf = globals()[classifier_map[args.method]](**classifiers_args[args.method])
    if args.proba:
      probs = clf.fit(train_x, train_y).predict_proba(test_x)
      preds = np.argmax(probs, axis=1)
      probabilities = np.concatenate((probabilities, probs[:, 0]))
    else:
      preds = clf.fit(train_x, train_y).predict(test_x)
    cm = confusion_matrix(test_y, preds)
    cma += cm.astype(int)
    predictions = np.concatenate((predictions, preds))
    scores = scores.append(pd.DataFrame([[accuracy_score(test_y, preds), balanced_accuracy_score(test_y, preds), 
        cm[0,0]/(cm[0,0]+cm[0,1]), cm[1,1]/(cm[1,0]+cm[1,1]), 
        matthews_corrcoef(test_y, preds), cm]], columns=columns_names, index=[fold]))
dfm_scores = pd.DataFrame(scores.mean(axis=0)).T
dfs_scores = pd.DataFrame(scores.std(axis=0)).T
df_scores = pd.DataFrame([f'{row[0]:.3f}±{row[1]:.3f}' for row in pd.concat([dfm_scores,dfs_scores], axis=0).T.values.tolist()]).T
df_scores.index=[f'{method}']
df_scores['CM'] = [cma]
df_scores.columns = columns_names
disp = ConfusionMatrixDisplay(confusion_matrix=cma,display_labels=encoder.inverse_transform(clf.classes_))
disp.plot()
plt.show() if args.display else None
print(bcolors.OKGREEN +  tabulate(df_scores, headers='keys', tablefmt='psql') + bcolors.ENDC)
if args.proba:
  df_results = pd.DataFrame({ 'gene': gg, 'label': yy, 'E_prob': probabilities, 'prediction': predictions})
  df_results = df_results.set_index(['gene'])
  df_results.to_csv('results.csv')
  print(df_results)
else:
  df_results = pd.DataFrame({ 'gene': gg, 'label': yy, 'prediction': predictions})
  df_results = df_results.set_index(['gene'])
  df_results.to_csv('results.csv')
  print(df_results)
