# -*- coding: utf-8 -*-
"""NodeClassificaytion.ipynb



Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NbmTr3i8M5BIn9UkOuNEY4xP81_EbbZh
"""

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    
import argparse
from operator import index
import numpy as np

parser = argparse.ArgumentParser(description='BIOMAT 2022 Workbench')
parser.add_argument('-a', "--attributes", dest='attributes', metavar='<attributes>', nargs="+", default=["BIO", "EMBED"], help='attributes to consider (default BIO EMBED, values BIO,GTEX,EMBED)', required=False)
parser.add_argument('-i', "--onlyattributes", dest='onlyattributes', metavar='<onlyattributes>', nargs="+", default=None, help='attributes to use (default None, values any list)', required=False)
parser.add_argument('-c', "--embeddir", dest='embeddir', metavar='<embedding-dir>', type=str, help='embedding directory (default embeddings)', default='embeddings', required=False)
parser.add_argument('-d', "--datadir", dest='datadir', metavar='<data-dir>', type=str, help='data directory (default datasets)', default='datasets', required=False)
parser.add_argument('-T', "--targetfile", dest='targetfile', metavar='<targetfile>', type=str, help='target filename (default node_targets.csv)', default='node_targets.csv', required=False)
parser.add_argument('-Y', "--targetpos", dest='targetpos', metavar='<targetpos>', type=int, nargs="+", default=[], help='targets positions (default [], values any list)', required=False)
parser.add_argument('-x', "--excludetargets", dest='excludetargets', metavar='<excludetargets>', nargs="+", default=[], help='targets to exlude (default None, values any list)', required=False)
parser.add_argument('-A', "--attrfile", dest='attrfile', metavar='<attrfile>', type=str, help='attribute filename (default node_attributes.csv)', default='node_attributes.csv', required=False)
parser.add_argument('-P', "--netfile", dest='netfile', metavar='<netfile>', type=str, help='network filename (default ppi.csv)', default='ppi.csv', required=False)
parser.add_argument('-E', "--essentials", dest='E_class', metavar='<essential-groups>', nargs="+", default=["CS0"], help='CS groups of essential genes (default: CS0, range: [CS0,...,CS9]', required=False)
parser.add_argument('-N', "--nonessenzials", dest='NE_class', metavar='<not-essential-groups>', nargs="+", default=["CS6", "CS7","CS8","CS9"], help='CS groups of non essential genes (default:  CS6 CS7 CS8 CS9], range: [CS0,...,CS9]', required=False)
parser.add_argument('-n', "--network", dest='network', metavar='<network>', type=str, help='network (default: PPI, choice: PPI|MET|MET+PPI)', choices=['PPI', 'MET', 'MET+PPI'], default='PPI', required=False)
parser.add_argument('-Z', "--normalize", dest='normalize', metavar='<normalize>', type=str, help='normalize mode (default: None, choice: None|zscore|minmax)', choices=[None, 'zscore', 'minmax'], default=None, required=False)
parser.add_argument('-e', "--embedder", dest='embedder', metavar='<embedder>', type=str, help='embedder name (default: RandNE, choice: RandNE|Node2Vec|GLEE|DeepWalk|HOPE|... any other)' , default='RandNE', required=False)
parser.add_argument('-s', "--embedsize", dest='embedsize', metavar='<embedsize>', type=int, help='embed size (default: 128)' , default='128', required=False)
parser.add_argument('-V', "--verbose", action='store_true', required=False)
parser.add_argument('-S', "--save-embedding", dest='saveembedding',  action='store_true', required=False)
parser.add_argument('-O', "--tuneparams", dest='tuneparams',  action='store_true', required=False)
parser.add_argument('-L', "--load-embedding", dest='loadembedding',  action='store_true', required=False)
parser.add_argument('-X', "--display", action='store_true', required=False)
parser.add_argument('-D', "--tocsv", action='store_true', required=False)
args = parser.parse_args()

seed=1

import warnings
warnings.filterwarnings('ignore')
import random
import pandas as pd
def set_seed(seed=1):
    random.seed(seed)
    np.random.seed(seed)

"""# Load the network"""

#@title  { run: "auto", form-width: "30%" }
network = args.network #@param ["PPI", "Met", "Met+PPI"]
import sys
if 'google.colab' in sys.modules:
	import tqdm.notebook as tq
else:
	import tqdm as tq
import pandas as pd
import networkx as nx
import os

datapath = args.datadir

"""# Read the labels
Load the label file, select the label type, and print label distribution
"""

from sklearn import preprocessing
from collections import Counter
import matplotlib.pyplot as plt

#@title Testo del titolo predefinito { run: "auto", form-width: "20%" }
targetfile = args.targetfile #@param {type:"string"}
import pandas as pd
import numpy as np
target_file = os.path.join(datapath,f'{targetfile}')
print(f'Loading target file "{target_file}"...')
df_target = pd.read_csv(target_file)
df_target['name'] = genes = df_target.index.values                                            # get genes with defined labels (E or NE)
df_target = df_target.reset_index()                                                           # reindex genes by consecutive integers
df_target['index'] = df_target.index
gene2idx_mapping = { v[1] : v[0]  for v in df_target[['index', 'name']].values }             # create mapping index by gene name
idx2gene_mapping = { v[0] : v[1]  for v in df_target[['index', 'name']].values }             # create mapping index by gene name
df_target = df_target.dropna(how='all')
exclude_target = args.excludetargets
df_target = df_target.drop(columns=exclude_target+['index']).set_index('name')                      # drop specified input colum target
df_target = df_target.iloc[:,args.targetpos] if args.targetpos != [] else df_target
selectedgenes = df_target.index.values
print(bcolors.OKGREEN + f'\t{len(selectedgenes)} genes with scores over a total of {len(genes)}' + bcolors.ENDC)

"""# Load attributes to be used
We identified three sets of attributes:
1. bio attributes, related to gene information (such as, expression, etc.)
1. GTEX-* attribute, additional biological information of genes 
Based on user selection, the node attributes are appended in a single matrix of attributes (`x`)

In the attribute matrix `x` there can be NaN or Infinite values. They are corrected as it follow:
+ NaN is replaced by the mean in the attribute range, 
+ Infinte value is replaced by the maximum in the range.

After Nan and Infinite values fixing, the attributes are normalized with Z-score or MinMax normalization functions.

At the end, only nodes (genes) with E or NE labels are selected for the classification
"""

#@title Choose attributes { form-width: "20%" }
import re
r = re.compile('^GTEX*')

if "BIO" in args.attributes:
  normalize_node = args.normalize #@param ["", "zscore", "minmax"]
  attr_file = os.path.join(datapath,args.attrfile)
  print(f'Loading attribute matrix "{attr_file}"...')
  x = pd.read_csv(attr_file)
  x = x.drop(columns=['id']) if 'id' in list(x.columns) else x
  #gtex_attributes = list(filter(r.match, x.columns)) 
  #bio_attributes = list(set(x.columns).difference(gtex_attributes)) if "BIO" in args.attributes else []
  #gtex_attributes = gtex_attributes if "GTEX" in args.attributes else [] 
  print(bcolors.OKGREEN + f'\tselecting attributes: {args.attributes} for {len(genes)} genes' + bcolors.ENDC)
  #x = x.filter(items=bio_attributes+gtex_attributes)
  if args.onlyattributes is not None:
  	x = x.filter(items=args.onlyattributes)
  print(bcolors.OKGREEN + f'\tfound {x.isnull().sum().sum()} NaN values and {np.isinf(x).values.sum()} Infinite values' + bcolors.ENDC)
  for col in x.columns[x.isna().any()].tolist():
    mean_value=x[col].mean()          # Replace NaNs in column with the mean of values in the same column
    if mean_value is not np.nan:
      x[col].fillna(value=mean_value, inplace=True)
    else:                             # otherwise, if the mean is NaN, remove the column
      x = x.drop(col, 1)
  if normalize_node == 'minmax':
    print(bcolors.OKGREEN + "\tgene attributes normalization (minmax)..." + bcolors.ENDC)
    x = (x-x.min())/(x.max()-x.min())
  elif normalize_node == 'zscore':
    print(bcolors.OKGREEN + "\tgene attributes normalization (zscore)..." + bcolors.ENDC)
    x = (x-x.mean())/x.std()
  selectedgenes = list(set(x.index.to_numpy()).intersection(set(selectedgenes)))
  print(bcolors.OKGREEN + f'\tgenes with attributes are {len(selectedgenes)}' + bcolors.ENDC)
  x = x.loc[selectedgenes]
  x = x[~x.index.duplicated(keep='first')]   # remove eventually duplicated index
  print(bcolors.OKGREEN + f'\tNew attribute matrix x{x.shape}' + bcolors.ENDC)
  x.to_csv("attrib.csv")
else:
  x = pd.DataFrame()

# Filter targets based on attributes
print(f'Filtering targets on genes with attribute...')
df_target = df_target.loc[selectedgenes]
print(bcolors.OKGREEN + f'\tfound {df_target.isnull().sum().sum()} NaN values and {np.isinf(df_target).values.sum()} Infinite values' + bcolors.ENDC)
for col in df_target.columns[df_target.isna().any()].tolist():
    mean_value=df_target[col].mean()          # Replace NaNs in column with the mean of values in the same column
    if mean_value is not np.nan:
      df_target[col].fillna(value=mean_value, inplace=True)
    else:                             # otherwise, if the mean is NaN, remove the column
      df_target = df_target.drop(col, 1)
print(bcolors.OKGREEN + f'\tNew target matrix y{df_target.shape}' + bcolors.ENDC)


"""# Load the PPI+MET network
The PPI networks is loaded from a CSV file, where
*   `from` is the column name for edge source (gene index)
*   `to` is the column name for edge target (gene index)
*   `weight` is the column name for edge weight

"""

if "EMBED" in args.attributes:
  """# Network embedding with Karateclub""" 

  from karateclub.node_embedding import *
  embeddername = args.embedder #@param ["RandNE", "Node2Vec", "GLEE", "DeepWalk"]
  if not embeddername in dir():
    raise Exception(bcolors.FAIL + f"{embeddername} is not an embedding method supported in karateclub" + bcolors.ENDC)
  print(f'Embedding with method "{embeddername}"...')
  embedfilename = os.path.join(args.embeddir,f'{network}_{embeddername}_{args.embedsize}.csv')
  if args.loadembedding:
    print(bcolors.OKGREEN + f'\tLoading precomputed embedding from file "{embedfilename}"' + bcolors.ENDC)
    embedding_df = pd.read_csv(embedfilename, index_col=0)
  else:
    import networkx as nx
    netfile = os.path.join(datapath,args.netfile)
    df_net = pd.read_csv(netfile, index_col=0)
    print(f'Loading "{network}" network file "{netfile}" ...')
    if network == "PPI":
      G = nx.Graph()
    else:
      G = nx.DiGraph()
    G.add_nodes_from(range(len(genes)))                                       # add all nodes (genes, also isolated ones)
    if 'weight' in list(df_net.columns):
      edge_list = [(gene2idx_mapping[v[0]], gene2idx_mapping[v[1]], v[2]) for v in list(df_net[['source','target', 'weight']].values)]      # get the edge list (with weights)
      G.add_weighted_edges_from(edge_list)                                      # add all edges
    else:
      edge_list = [(gene2idx_mapping[v[0]], gene2idx_mapping[v[1]]) for v in list(df_net[['source','target']].values)]    # get the edge list (with weights)
      G.add_edges_from(edge_list)                                      # add all edges
    print(bcolors.OKGREEN + "\t" + nx.info(G)  + bcolors.ENDC)
    print(bcolors.OKGREEN + f'\tThere are {len(list(nx.isolates(G)))} isolated genes' + bcolors.ENDC)
    print(bcolors.OKGREEN + f'\tGraph {"is" if nx.is_weighted(G) else "is not"} weighted' + bcolors.ENDC)
    print(bcolors.OKGREEN + f'\tGraph {"is" if nx.is_directed(G) else "is not"} directed' + bcolors.ENDC)
    embedder = globals()[embeddername](dimensions=args.embedsize)
    embedder.fit(G)
    embedding = embedder.get_embedding()
    embedding_df = pd.DataFrame(embedding, columns = [f'{embeddername}_' + str(i + 1)  for i in range(embedding.shape[1])])
    embedding_df['name'] = [idx2gene_mapping[item] for item in embedding_df.index.values]
    embedding_df = embedding_df.set_index('name')
  if args.saveembedding:
    embedding_df.to_csv(embedfilename)
    print(bcolors.OKGREEN + f'\Saving embedding to file "{embedfilename}"' + bcolors.ENDC)
  embedding_df = embedding_df.loc[selectedgenes]                                     # keep only embeddings of selected genes (those with labels)
  x = pd.concat([embedding_df, x], axis=1)

  print(bcolors.OKGREEN + f'\tNew attribute matrix x{x.shape}' + bcolors.ENDC)


X = x.to_numpy()
y = df_target.to_numpy()
target_pos = 0
#y = df_target.iloc[:,target_pos].to_numpy()

"""# OPtuna study
"""
import optuna
import lightgbm as lgb
from sklearn.model_selection import KFold, train_test_split
from sklearn.multioutput import MultiOutputRegressor, RegressorChain
from sklearn.metrics import *

regressor_args = {'random_state' : seed}
#
# Optimal for Kidney
#
#regressor_args = {
#    'verbosity': -1,
#    'max_bin': 343,
#    'num_leaves': 320,
#    'lambda_l1': 1.3075993279613565,
#    'lambda_l2': 9.901908709342898,
#    'feature_fraction': 0.4677435312483704,
#    'bagging_fraction': 0.6458145307344503,
#    'bagging_freq': 5,
#    'min_data_in_leaf': 49,
#    'min_child_samples': 44,
#    'sub_feature': 0.35181769039281496,
#    'sub_row': 0.3962673628733871}
if args.tuneparams:
  print(f'\Optimizing method ...')
  def objective_xgb(trial):
    """
    Objective function to tune an `LGBMRegressor` model.
    """
    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25, random_state=42)

    params = {
        'verbosity': -1,
        'force_row_wise': True,
        'max_depth': -1,
        
        'max_bin': trial.suggest_int('max_bin', 1, 512),
        'num_leaves': trial.suggest_int('num_leaves', 2, 512),
        
        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),
        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),
        
        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),

        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        
        'sub_feature': trial.suggest_uniform('sub_feature', 0.0, 1.0),
        'sub_row': trial.suggest_uniform('sub_row', 0.0, 1.0)
    }

    model = MultiOutputRegressor(lgb.LGBMRegressor(
        boosting_type='gbdt',
        objective="regression",
        random_state=1,
        **params
    ))

    yhat = model.fit(train_x,train_y).predict(valid_x)
    return r2_score(valid_y, yhat, multioutput='variance_weighted')

  study = optuna.create_study(direction="maximize")
  study.optimize(objective_xgb, n_trials=50)
  study.best_params

  print("Number of finished trials: {}".format(len(study.trials)))

  print("Best trial:")
  trial = study.best_trial

  print("  Value: {}".format(trial.value))

  print("  Params: ")
  for key, value in trial.params.items():
      print("    {}: {}".format(key, value))



"""# k-fold cross validation with: SVM, RF, XGB, MLP, RUS

"""

#@title Choose classifier { run: "auto", form-width: "20%" }
from tqdm import tqdm
from tabulate import tabulate
set_seed(seed)
nfolds = 5
kf = KFold(n_splits=nfolds, shuffle=True, random_state=seed)
accuracies, mccs = [], []


print(X.shape, y.shape)
predictions = np.empty(shape=[0, y.shape[1]])
#predictions = np.array([])

columns_names = ["MSE", "MAE", "R2"]
scores = pd.DataFrame(columns=columns_names)
print(f'Regression with method LGBM and params {regressor_args}...')
for fold, (train_idx, test_idx) in enumerate(tqdm(kf.split(np.arange(len(X)), y), total=kf.get_n_splits(), desc=bcolors.OKGREEN +  f"{nfolds}-fold")):
    train_x, train_y, test_x, test_y = X[train_idx], y[train_idx], X[test_idx], y[test_idx],
    preds = MultiOutputRegressor(lgb.LGBMRegressor(**regressor_args)).fit(train_x, train_y).predict(test_x) 
    predictions = np.concatenate((predictions, preds))
    scores = scores.append(pd.DataFrame([[mean_squared_error(test_y, preds), mean_absolute_error(test_y, preds),
         r2_score(test_y, preds, multioutput='variance_weighted')]], 
                                columns=columns_names, index=[fold]))
dfm_scores = pd.DataFrame(scores.mean(axis=0)).T
dfs_scores = pd.DataFrame(scores.std(axis=0)).T
df_scores = pd.DataFrame([f'{row[0]:.3f}Â±{row[1]:.3f}' for row in pd.concat([dfm_scores,dfs_scores], axis=0).T.values.tolist()]).T
df_scores.index=['LGBM']
df_scores.columns = columns_names
print(bcolors.OKGREEN +  tabulate(df_scores, headers='keys', tablefmt='psql') + bcolors.ENDC)
