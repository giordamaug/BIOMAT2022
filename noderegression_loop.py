# -*- coding: utf-8 -*-
"""NodeClassificaytion.ipynb



Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NbmTr3i8M5BIn9UkOuNEY4xP81_EbbZh
"""

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    
import argparse
from operator import index
import numpy as np

parser = argparse.ArgumentParser(description='BIOMAT 2022 Workbench')
parser.add_argument('-a', "--attributes", dest='attributes', metavar='<attributes>', nargs="+", default=["BIO", "EMBED"], help='attributes to consider (default BIO EMBED, values BIO,GTEX,EMBED)', required=False)
parser.add_argument('-i', "--onlyattributes", dest='onlyattributes', metavar='<onlyattributes>', nargs="+", default=None, help='attributes to use (default None, values any list)', required=False)
parser.add_argument('-c', "--embeddir", dest='embeddir', metavar='<embedding-dir>', type=str, help='embedding directory (default embeddings)', default='embeddings', required=False)
parser.add_argument('-d', "--datadir", dest='datadir', metavar='<data-dir>', type=str, help='data directory (default datasets)', default='datasets', required=False)
parser.add_argument('-T', "--targetfile", dest='targetfile', metavar='<targetfile>', type=str, help='target filename (default node_targets.csv)', default='node_targets.csv', required=False)
parser.add_argument('-A', "--attrfile", dest='attrfile', metavar='<attrfile>', type=str, help='attribute filename (default node_attributes.csv)', default='node_attributes.csv', required=False)
parser.add_argument('-P', "--netfile", dest='netfile', metavar='<netfile>', type=str, help='network filename (default ppi.csv)', default='ppi.csv', required=False)
parser.add_argument('-E', "--essentials", dest='E_class', metavar='<essential-groups>', nargs="+", default=["CS0"], help='CS groups of essential genes (default: CS0, range: [CS0,...,CS9]', required=False)
parser.add_argument('-N', "--nonessenzials", dest='NE_class', metavar='<not-essential-groups>', nargs="+", default=["CS6", "CS7","CS8","CS9"], help='CS groups of non essential genes (default:  CS6 CS7 CS8 CS9], range: [CS0,...,CS9]', required=False)
parser.add_argument('-n', "--network", dest='network', metavar='<network>', type=str, help='network (default: PPI, choice: PPI|MET|MET+PPI)', choices=['PPI', 'MET', 'MET+PPI'], default='PPI', required=False)
parser.add_argument('-Z', "--normalize", dest='normalize', metavar='<normalize>', type=str, help='normalize mode (default: None, choice: None|zscore|minmax)', choices=[None, 'zscore', 'minmax'], default=None, required=False)
parser.add_argument('-e', "--embedder", dest='embedder', metavar='<embedder>', type=str, help='embedder name (default: RandNE, choice: RandNE|Node2Vec|GLEE|DeepWalk|HOPE|... any other)' , default='RandNE', required=False)
parser.add_argument('-s', "--embedsize", dest='embedsize', metavar='<embedsize>', type=int, help='embed size (default: 128)' , default='128', required=False)
parser.add_argument('-m', "--method", dest='method', metavar='<method>', type=str, help='classifier name (default: RF, choice: RF|XGB|LGBM)', choices=['RF', 'XGB', 'LGBM', 'SVM'], default='RF', required=False)
parser.add_argument('-V', "--verbose", action='store_true', required=False)
parser.add_argument('-S', "--save-embedding", dest='saveembedding',  action='store_true', required=False)
parser.add_argument('-O', "--tuneparams", dest='tuneparams',  action='store_true', required=False)
parser.add_argument('-L', "--load-embedding", dest='loadembedding',  action='store_true', required=False)
parser.add_argument('-X', "--display", action='store_true', required=False)
parser.add_argument('-D', "--tocsv", action='store_true', required=False)
args = parser.parse_args()

seed=1

classifier_map = {'RF' : 'RandomForestRegressor', 
                  'SVM' : 'SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1)', 
                  'XGB': 'XGBRegressor',
                  'LGBM': 'LGBMRegressor'}

classifiers_args = {
  'RF' : {'random_state' : seed, 'class_weight': 'balanced'}, 
  'XGB': {'random_state' : seed, 'eval_metric' : 'logloss', 'scale_pos_weight' : 0.2},
  'LGBM': {'random_state' : seed, 'class_weight': 'balanced'},
}


import warnings
warnings.filterwarnings('ignore')
import random
import pandas as pd
def set_seed(seed=1):
    random.seed(seed)
    np.random.seed(seed)

"""# Load the network"""

#@title  { run: "auto", form-width: "30%" }
network = args.network #@param ["PPI", "Met", "Met+PPI"]
import sys
if 'google.colab' in sys.modules:
	import tqdm.notebook as tq
else:
	import tqdm as tq
import pandas as pd
import networkx as nx
import os

datapath = args.datadir

"""# Read the labels
Load the label file, select the label type, and print label distribution
"""

from sklearn import preprocessing
from collections import Counter
import matplotlib.pyplot as plt

#@title Testo del titolo predefinito { run: "auto", form-width: "20%" }
targetfile = args.targetfile #@param {type:"string"}
import pandas as pd
import numpy as np
target_file = os.path.join(datapath,f'{targetfile}')
print(f'Loading target file "{target_file}"...')
df_target = pd.read_csv(target_file)
df_target['name'] = genes = df_target.index.values                                            # get genes with defined labels (E or NE)
df_target = df_target.reset_index()                                                           # reindex genes by consecutive integers
df_target['index'] = df_target.index
gene2idx_mapping = { v[1] : v[0]  for v in df_target[['index', 'name']].values }             # create mapping index by gene name
idx2gene_mapping = { v[0] : v[1]  for v in df_target[['index', 'name']].values }             # create mapping index by gene name
df_target = df_target.dropna(how='all')
df_target = df_target.drop(columns=['index']).set_index('name')                      # drop specified input colum target
selectedgenes = df_target.index.values
print(bcolors.OKGREEN + f'\t{len(selectedgenes)} genes with scores over a total of {len(genes)}' + bcolors.ENDC)

"""# Load attributes to be used
We identified three sets of attributes:
1. bio attributes, related to gene information (such as, expression, etc.)
1. GTEX-* attribute, additional biological information of genes 
Based on user selection, the node attributes are appended in a single matrix of attributes (`x`)

In the attribute matrix `x` there can be NaN or Infinite values. They are corrected as it follow:
+ NaN is replaced by the mean in the attribute range, 
+ Infinte value is replaced by the maximum in the range.

After Nan and Infinite values fixing, the attributes are normalized with Z-score or MinMax normalization functions.

At the end, only nodes (genes) with E or NE labels are selected for the classification
"""

#@title Choose attributes { form-width: "20%" }
import re
r = re.compile('^GTEX*')

if "BIO" in args.attributes:
  normalize_node = args.normalize #@param ["", "zscore", "minmax"]
  attr_file = os.path.join(datapath,args.attrfile)
  print(f'Loading attribute matrix "{attr_file}"...')
  x = pd.read_csv(attr_file)
  x = x.drop(columns=['id']) if 'id' in list(x.columns) else x
  #gtex_attributes = list(filter(r.match, x.columns)) 
  #bio_attributes = list(set(x.columns).difference(gtex_attributes)) if "BIO" in args.attributes else []
  #gtex_attributes = gtex_attributes if "GTEX" in args.attributes else [] 
  print(bcolors.OKGREEN + f'\tselecting attributes: {args.attributes} for {len(genes)} genes' + bcolors.ENDC)
  #x = x.filter(items=bio_attributes+gtex_attributes)
  if args.onlyattributes is not None:
  	x = x.filter(items=args.onlyattributes)
  print(bcolors.OKGREEN + f'\tfound {x.isnull().sum().sum()} NaN values and {np.isinf(x).values.sum()} Infinite values' + bcolors.ENDC)
  for col in x.columns[x.isna().any()].tolist():
    mean_value=x[col].mean()          # Replace NaNs in column with the mean of values in the same column
    if mean_value is not np.nan:
      x[col].fillna(value=mean_value, inplace=True)
    else:                             # otherwise, if the mean is NaN, remove the column
      x = x.drop(col, 1)
  if normalize_node == 'minmax':
    print(bcolors.OKGREEN + "\tgene attributes normalization (minmax)..." + bcolors.ENDC)
    x = (x-x.min())/(x.max()-x.min())
  elif normalize_node == 'zscore':
    print(bcolors.OKGREEN + "\tgene attributes normalization (zscore)..." + bcolors.ENDC)
    x = (x-x.mean())/x.std()
  selectedgenes = list(set(x.index.to_numpy()).intersection(set(selectedgenes)))
  print(bcolors.OKGREEN + f'\tgenes with attributes are {len(selectedgenes)}' + bcolors.ENDC)
  x = x.loc[selectedgenes]
  x = x[~x.index.duplicated(keep='first')]   # remove eventually duplicated index
  print(bcolors.OKGREEN + f'\tNew attribute matrix x{x.shape}' + bcolors.ENDC)
  x.to_csv("attrib.csv")
else:
  x = pd.DataFrame()

# Filter targets based on attributes
print(f'Filtering targets on genes with attribute...')
df_target = df_target.loc[selectedgenes]
print(bcolors.OKGREEN + f'\tfound {df_target.isnull().sum().sum()} NaN values and {np.isinf(df_target).values.sum()} Infinite values' + bcolors.ENDC)
for col in df_target.columns[df_target.isna().any()].tolist():
    mean_value=df_target[col].mean()          # Replace NaNs in column with the mean of values in the same column
    if mean_value is not np.nan:
      df_target[col].fillna(value=mean_value, inplace=True)
    else:                             # otherwise, if the mean is NaN, remove the column
      df_target = df_target.drop(col, 1)
print(bcolors.OKGREEN + f'\tNew target matrix y{df_target.shape}' + bcolors.ENDC)


"""# Load the PPI+MET network
The PPI networks is loaded from a CSV file, where
*   `from` is the column name for edge source (gene index)
*   `to` is the column name for edge target (gene index)
*   `weight` is the column name for edge weight

"""

if "EMBED" in args.attributes:
  """# Network embedding with Karateclub""" 

  from karateclub.node_embedding import *
  embeddername = args.embedder #@param ["RandNE", "Node2Vec", "GLEE", "DeepWalk"]
  if not embeddername in dir():
    raise Exception(bcolors.FAIL + f"{embeddername} is not an embedding method supported in karateclub" + bcolors.ENDC)
  print(f'Embedding with method "{embeddername}"...')
  embedfilename = os.path.join(args.embeddir,f'{network}_{embeddername}_{args.embedsize}.csv')
  if args.loadembedding:
    print(bcolors.OKGREEN + f'\tLoading precomputed embedding from file "{embedfilename}"' + bcolors.ENDC)
    embedding_df = pd.read_csv(embedfilename, index_col=0)
  else:
    import networkx as nx
    netfile = os.path.join(datapath,args.netfile)
    df_net = pd.read_csv(netfile, index_col=0)
    print(f'Loading "{network}" network file "{netfile}" ...')
    if network == "PPI":
      G = nx.Graph()
    else:
      G = nx.DiGraph()
    G.add_nodes_from(range(len(genes)))                                       # add all nodes (genes, also isolated ones)
    if 'weight' in list(df_net.columns):
      edge_list = [(gene2idx_mapping[v[0]], gene2idx_mapping[v[1]], v[2]) for v in list(df_net[['source','target', 'weight']].values)]      # get the edge list (with weights)
      G.add_weighted_edges_from(edge_list)                                      # add all edges
    else:
      edge_list = [(gene2idx_mapping[v[0]], gene2idx_mapping[v[1]]) for v in list(df_net[['source','target']].values)]    # get the edge list (with weights)
      G.add_edges_from(edge_list)                                      # add all edges
    print(bcolors.OKGREEN + "\t" + nx.info(G)  + bcolors.ENDC)
    print(bcolors.OKGREEN + f'\tThere are {len(list(nx.isolates(G)))} isolated genes' + bcolors.ENDC)
    print(bcolors.OKGREEN + f'\tGraph {"is" if nx.is_weighted(G) else "is not"} weighted' + bcolors.ENDC)
    print(bcolors.OKGREEN + f'\tGraph {"is" if nx.is_directed(G) else "is not"} directed' + bcolors.ENDC)
    embedder = globals()[embeddername](dimensions=args.embedsize)
    embedder.fit(G)
    embedding = embedder.get_embedding()
    embedding_df = pd.DataFrame(embedding, columns = [f'{embeddername}_' + str(i + 1)  for i in range(embedding.shape[1])])
    embedding_df['name'] = [idx2gene_mapping[item] for item in embedding_df.index.values]
    embedding_df = embedding_df.set_index('name')
  if args.saveembedding:
    embedding_df.to_csv(embedfilename)
    print(bcolors.OKGREEN + f'\Saving embedding to file "{embedfilename}"' + bcolors.ENDC)
  embedding_df = embedding_df.loc[selectedgenes]                                     # keep only embeddings of selected genes (those with labels)
  x = pd.concat([embedding_df, x], axis=1)

  print(bcolors.OKGREEN + f'\tNew attribute matrix x{x.shape}' + bcolors.ENDC)

"""# k-fold cross validation with: SVM, RF, XGB, MLP, RUS

"""

#@title Choose classifier { run: "auto", form-width: "20%" }
method = args.method #@param ["SVM", "XGB", "RF", "MLP", "RUS", "LGB"]
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor, RegressorChain
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.svm import SVR
from sklearn.model_selection import KFold, train_test_split
from tqdm import tqdm
from sklearn.metrics import *
from tabulate import tabulate
set_seed(seed)
nfolds = 5
kf = KFold(n_splits=nfolds, shuffle=True, random_state=seed)
accuracies, mccs = [], []
X = x.to_numpy()

predictions = np.array([])

columns_names = ["MSE", "MAE", "R2"]
scores = pd.DataFrame(columns=columns_names)
print(f'Regression with method "{method}"...')
for i in range(len(df_target.columns)):
  y = df_target.iloc[:,i].to_numpy()
  for fold, (train_idx, test_idx) in enumerate(tqdm(kf.split(np.arange(len(X)), y), total=kf.get_n_splits(), desc=bcolors.OKGREEN +  f"{nfolds}-fold")):
      train_x, train_y, test_x, test_y = X[train_idx], y[train_idx], X[test_idx], y[test_idx],
      if args.method == 'RF':
        preds =RandomForestRegressor().fit(train_x, train_y).predict(test_x)
      elif args.method == 'XGB':
        preds = XGBRegressor().fit(train_x, train_y).predict(test_x)
        #preds = RegressorChain(base_estimator=XGBRegressor(objective='reg:squarederror')).fit(train_x, train_y).predict(test_x)
      elif args.method == 'LGBM':
        preds = LGBMRegressor().fit(train_x, train_y).predict(test_x)
      elif args.method == 'SVM':
        preds = SVR().fit(train_x, train_y).predict(test_x)
      else:
        raise Exception("Wrong regressor method")
      predictions = np.concatenate((predictions, preds))
      scores = scores.append(pd.DataFrame([[mean_squared_error(test_y, preds), mean_absolute_error(test_y, preds),
           r2_score(test_y, preds, multioutput='variance_weighted')]], 
                                  columns=columns_names, index=[fold]))
  dfm_scores = pd.DataFrame(scores.mean(axis=0)).T
  dfs_scores = pd.DataFrame(scores.std(axis=0)).T
  df_scores = pd.DataFrame([f'{row[0]:.3f}Â±{row[1]:.3f}' for row in pd.concat([dfm_scores,dfs_scores], axis=0).T.values.tolist()]).T
  df_scores.index=[f'{method}_{i}']
  df_scores.columns = columns_names
  print(bcolors.OKGREEN +  tabulate(df_scores, headers='keys', tablefmt='psql') + bcolors.ENDC)
